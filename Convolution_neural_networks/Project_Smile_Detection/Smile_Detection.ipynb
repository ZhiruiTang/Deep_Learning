{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e9a9007b",
   "metadata": {},
   "source": [
    "In this notebook, we will perform the project smile detection.\n",
    "\n",
    "The basic idea is that we train a LetNet network by dataset SMILES which cantains 13,165 images of smile or not smile face. After training the model, we access our webcam/video file and apply smile detection to each frame. \n",
    "Faces are detected by Haar cascade face detector and then extracted from images to feed the LeNet network.\n",
    "\n",
    "\n",
    "Note: there are 13,165 images in the dataset, 9,475 of these examples are not smiling while only 3,690 belong to the smiling class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "680bab31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import load_model\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "from imutils import paths\n",
    "import numpy as np\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f39ca347",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "labels = []\n",
    "\n",
    "DATASET_PATH = \"SMILEsmileD/SMILEs\" \n",
    "\n",
    "for imagePath in sorted(list(paths.list_images(DATASET_PATH))):\n",
    "    # load the image, pre-process it, and store it in the data list\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    image = imutils.resize(image, width=28)\n",
    "    image = img_to_array(image)\n",
    "    data.append(image)\n",
    "    \n",
    "    # extract the class label from the image path and update the labels list\n",
    "    label = imagePath.split(os.path.sep)[-3]\n",
    "    lable = \"smiling\" if label == \"positives\" else \"not_smiling\"\n",
    "    labels.append(label)\n",
    "\"\"\"\n",
    "The SMILES dataset stores smiling faces in the  SMILES/positives/positives7 subdirectory \n",
    "while not smiling faces live in the SMILES/negatives/negatives7 subdirectory.\n",
    "\n",
    "Therefore, given the path to an image:\n",
    "\n",
    "    SMILEs/positives/positives7/10007.jpg\n",
    "    \n",
    "We can extract the class label by splitting on the image path separator \n",
    "and grabbing the third-to-last subdirectory: positives. \n",
    "\"\"\"    \n",
    "#print(type(labels[1]))\n",
    "data = np.array(data, dtype=\"float\") / 255.0\n",
    "labels = np.array(labels)\n",
    "#print(type(labels[1]))\n",
    "#print(labels[:5])\n",
    "le = LabelEncoder().fit(labels)\n",
    "labels = np_utils.to_categorical(le.transform(labels), 2)\n",
    "#print(type(labels))\n",
    "#print(type(labels[1,1]))\n",
    "#print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eaaa56",
   "metadata": {},
   "source": [
    "As mentioned before, the data is unbalance, namley the number of negative images is much greater than that of positive. We could handle it by weighting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "271a6d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute prorportions of each catogory\n",
    "classTotals = labels.sum(axis=0)\n",
    "classWeight = classTotals.max() / classTotals\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "label contains two kinds of element, [1, 0] and [0, 1]. Thus, the number of each catogory \n",
    "can be count easily by sum.\n",
    "\n",
    "The weight is approx. [1, 2.56] for [negative, positive]\n",
    "This weighting implies that our network will treat \n",
    "every instance of “smiling” as 2.56 instances of “not smiling” \n",
    "and helps combat the class imbalance issue \n",
    "by amplifying the per-instance loss \n",
    "by a larger weight when seeing “smiling” examples.\n",
    "\"\"\"\n",
    "\n",
    "#split the whole data to train and test data\n",
    "(trainX, testX, trainY, testY) = train_test_split(data,\n",
    "    labels, test_size=0.20, stratify=labels, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76aa9a5b",
   "metadata": {},
   "source": [
    "### Train LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "72e30d32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] compiling model...\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               819712    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 1026      \n",
      "=================================================================\n",
      "Total params: 839,554\n",
      "Trainable params: 839,554\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-03 19:26:13.806280: I tensorflow/core/platform/cpu_feature_guard.cc:145] This TensorFlow binary is optimized with Intel(R) MKL-DNN to use the following CPU instructions in performance critical operations:  SSE4.1 SSE4.2 AVX AVX2 FMA\n",
      "To enable them in non-MKL-DNN operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-03 19:26:13.806939: I tensorflow/core/common_runtime/process_util.cc:115] Creating new thread pool with default inter op setting: 8. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D\n",
    "from keras.layers.core import Flatten, Dense, Dropout\n",
    "from keras import optimizers\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#The number of categroies of letter \n",
    "Num_classes = 2\n",
    "\n",
    "print(\"[INFO] compiling model...\")\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3,3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(Num_classes, activation='sigmoid')) #2 classes not need softmax\n",
    "\n",
    "model.summary()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c5e5b2",
   "metadata": {},
   "source": [
    "### Compile and train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48cee3cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"[INFO] compiling model...\")\n",
    "# model.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n",
    "\n",
    "# Epochs = 15\n",
    "# print(\"[INFO] training network...\")\n",
    "# H = model.fit(trainX, trainY, validation_data=(testX, testY),\n",
    "#               class_weight=classWeight, batch_size=64, epochs=Epochs, verbose=1)\n",
    "# \"\"\"\n",
    "# class_weight is used for imbalance issue\n",
    "\n",
    "# Explanation:\n",
    "# https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras\n",
    "\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b678dca9",
   "metadata": {},
   "source": [
    "### Evaluation and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9e11cc91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"[INFO] evaluating network...\")\n",
    "# predictions = model.predict(testX, batch_size=64)\n",
    "# print(classification_report(testY.argmax(axis=1),\n",
    "#     predictions.argmax(axis=1), target_names=le.classes_))\n",
    "\n",
    "# MODEL_FILENAME = \"Smile_Detection.hdf5\"\n",
    "# print(\"[INFO] saving model...\")\n",
    "# model.save(MODEL_FILENAME)\n",
    "# print(\"[INFO] finished\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcefe8a4",
   "metadata": {},
   "source": [
    "### curve for perfomance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c588a37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.style.use(\"ggplot\")\n",
    "# plt.figure()\n",
    "# plt.plot(np.arange(0, Epochs), H.history['loss'], label=\"train_loss\")\n",
    "# plt.plot(np.arange(0, Epochs), H.history['val_loss'], label=\"val_loss\")\n",
    "# plt.plot(np.arange(0, Epochs), H.history['accuracy'], label=\"acc\")\n",
    "# plt.plot(np.arange(0, Epochs), H.history['val_accuracy'], label=\"val_loss\")\n",
    "# plt.title(\"Training Loss and Accuracy\")\n",
    "# plt.xlabel(\"Epoch #\")\n",
    "# plt.ylabel(\"Loss/Accuracy\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# \"\"\"\n",
    "# Note: H.history['accuracy'] or H.history['acc']\n",
    "# denpends on model.compile(... ,metrics=[\"accuracy\"]) or\n",
    "# model.compile(... ,metrics=[\"acc\"])\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1d4c86",
   "metadata": {},
   "source": [
    "### Running the Smile CNN in Real-time\n",
    "We've trained our model, then we need access webcam/video file and detect smile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e144c39",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "CASCADE = \"haarcascade_frontalface_default.xml\"\n",
    "MODEL = \"Smile_Detection.hdf5\"\n",
    "VIDEO = \"Smile_video.mp4\"\n",
    "\n",
    "detector = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "model = load_model(MODEL)\n",
    "camera = cv2.VideoCapture(0) #from webcam\n",
    "#camera = cv2.VideoCapture(VIDEO) #from video file\n",
    "\n",
    "while True:\n",
    "    # grab the current frame\n",
    "    (grabbed, frame) = camera.read()\n",
    "    \n",
    "    # if we are viewing a video and we did not grab a frame, \n",
    "    #then we have reached the end of the video\n",
    "    if VIDEO != \"\" and not grabbed:\n",
    "        break\n",
    "    # Otherwise    \n",
    "    # resize the frame, convert it to grayscale, and then clone the\n",
    "    # original frame so we can draw on it later in the program\n",
    "    frame = imutils.resize(frame, width=300)\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    frameClone = frame.copy()\n",
    "    \n",
    "    # detect faces in the input frame, then clone the frame \n",
    "    # so that we can draw on it\n",
    "    rects = detector.detectMultiScale(gray, scaleFactor=1.1,\n",
    "                                      minNeighbors=5, minSize=(30, 30),\n",
    "                                      flags=cv2.CASCADE_SCALE_IMAGE)\n",
    "\n",
    "    # loop over the face bounding boxes\n",
    "    for (fX, fY, fW, fH) in rects:\n",
    "        roi = gray[fY:fY + fH, fX:fX + fW]\n",
    "        roi = cv2.resize(roi, (28, 28))\n",
    "        roi = roi.astype(\"float\") / 255.0\n",
    "        roi = img_to_array(roi)\n",
    "        roi = np.expand_dims(roi, axis=0)\n",
    "        \n",
    "        (notSmiling, smiling) = model.predict(roi)[0]\n",
    "        label = \"Smiling\"  if smiling > notSmiling else \"Not_Smiling\"\n",
    "        if smiling > notSmiling:\n",
    "        # display the label and bounding box rectangle on the output # frame\n",
    "            cv2.putText(frameClone, label, (fX, fY - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)\n",
    "            cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),\n",
    "                      (0, 0, 255), 2)\n",
    "        else:\n",
    "            cv2.putText(frameClone, label, (fX, fY - 10),\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 255, 0), 2)\n",
    "            cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),\n",
    "                      (0, 255, 0), 2)\n",
    "        # show our detected faces along with smiling/not smiling labels\n",
    "        cv2.imshow(\"Face\", frameClone)\n",
    "        \n",
    "         # if the ’q’ key is pressed, stop the loop\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "The .detectMultiScale method handles detecting the bounding box \n",
    "(x,y)-coordinates of faces in the frame\n",
    "    \n",
    "It returns a list of 4-tuples that make up the rectangle \n",
    "that bounds the face in the frame. \n",
    "The first two values in this list are the starting (x, y)-coordinates. \n",
    "The second two values in the rects list are\n",
    "the width and height of the bounding box, respectively.\n",
    "\"\"\"\n",
    "\n",
    "\"\"\"\n",
    "Inside the for loop,\n",
    "extract the ROI of the face from the grayscale image,\n",
    "resize it to a fixed 28x28 pixels, and then prepare the\n",
    "ROI for classification via the CNN\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f5e7f34",
   "metadata": {},
   "source": [
    "`CascadeClassifier` function requires `opencv-contrib-python`. While we cannot install by conda directly. \n",
    "\n",
    "Hence, we need install `pip` in conda environment `keras_env`\n",
    "\n",
    "So, firstly, activate the environment by\n",
    "`conda activate keras_env`.\n",
    "\n",
    "Then, go to the envrionment, the path can be found by\n",
    "`which python`\n",
    "\n",
    "It returns `/opt/anaconda3/envs/keras_env/bin/python`\n",
    "\n",
    "Next, we can install `opencv-contrib-python` by `/opt/anaconda3/envs/keras_env/bin/pip install opencv-contrib-python`\n",
    "\n",
    "After that, restart this notebook. Finally, everything is find."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b87906ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "camera.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb17b59f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
